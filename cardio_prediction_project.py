# -*- coding: utf-8 -*-
"""Cardio_Prediction_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19BeULyeyXvho_GX0TjqYkx0yW8iSr_Yl
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import io

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, roc_auc_score, confusion_matrix,
                             classification_report, roc_curve)


sns.set(style="whitegrid")
print("Libraries imported successfully.")

"""**UPLOAD AND READ CSV**"""

from google.colab import files

print("Please upload the csv file:")
uploaded = files.upload()

# Get the filename
filename = list(uploaded.keys())[0]

# Load into DataFrame
df = pd.read_csv(io.BytesIO(uploaded[filename]), sep=';')

# Display basic info
print("Dataset Info")
print(f"Dataset Shape: {df.shape}")

print("First 5 Rows:")
print(df.head())

print("Column Names:")
print(df.columns.tolist())

print("Data Description:")
print(df.describe())

"""**DATA CLEANING**"""

print(f"Shape before cleaning: {df.shape}")

# Check for missing values
print("Missing Values:")
print(df.isnull().sum())

# Remove duplicate rows
df.drop_duplicates(inplace=True)
print(f"Shape after dropping duplicates: {df.shape}")

# Remove impossible / non-positive values
df = df[(df['age'] > 0) & (df['height'] > 0) & (df['weight'] > 0)]
print(f"Shape after removing non-positive age/height/weight: {df.shape}")

# Remove unrealistic blood pressure values
# Criteria: ap_hi >= 80, ap_lo >= 60, ap_lo <= ap_hi
df = df[(df['ap_hi'] >= 80) & (df['ap_lo'] >= 60) & (df['ap_lo'] <= df['ap_hi'])]
print(f"Shape after cleaning blood pressure logic: {df.shape}")

#Remove outliers using IQR method
numeric_columns = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']

for col in numeric_columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Filter data
    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
    print(f"Shape after IQR removal on {col}: {df.shape}")

print(f"Shape After cleaning: {df.shape}")

"""**FEATURE ENGINEERING**"""

#Convert age from days to years
df['age'] = df['age'] // 365
print("Converted 'age' from days to years.")

# Check the updated column
df.head()

# Calculate BMI
# BMI = weight (kg) / height (m)^2
# Height is in cm, so we divide by 100 to get meters
df['BMI'] = df['weight'] / ((df['height'] / 100) ** 2)

# Create BMI_category
# 0: Underweight (<18.5), 1: Normal (18.5-24.9), 2: Overweight (25-29.9), 3: Obese (>=30)
def categorize_bmi(bmi):
    if bmi < 18.5:
        return 0
    elif 18.5 <= bmi < 25:
        return 1
    elif 25 <= bmi < 30:
        return 2
    else:
        return 3

df['BMI_category'] = df['BMI'].apply(categorize_bmi)

print(f"Shape after adding BMI and BMI_category: {df.shape}")

print(df[['age', 'height', 'weight', 'BMI', 'BMI_category']].head())

"""**ONE-HOT ENCODING**"""

# Columns to encode
categorical_cols = ['gender', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'BMI_category']

# Apply One-Hot Encoding
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

print(f"Shape after One-Hot Encoding: {df.shape}")
print("New columns:", df.columns.tolist())

"""**Exploratory Data Analysis (EDA)**"""

# General Overview of Transformed Data
print(" Data Info (Transformed):")
print(df.info())

print(" Descriptive Statistics:")
print(df.describe().T)

# Correlation Heatmap
# This shows how every specific dummy variable correlates with Heart Disease
plt.figure(figsize=(16, 12))
correlation_matrix = df.corr()

# Mask upper triangle to reduce clutter
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',
            mask=mask, linewidths=0.5, vmin=-0.2, vmax=0.2)
plt.title('Correlation Heatmap (Including One-Hot Encoded Features)')
plt.show()

# Print specific correlations with Target
print("\nTop Correlations with 'cardio':")
print(correlation_matrix['cardio'].sort_values(ascending=False))

# 3. Distribution of Key Numerical Features
# We focus on the continuous variables: Age, BMI, Blood Pressure
numerical_features = ['age', 'height', 'weight', 'BMI', 'ap_hi', 'ap_lo']

plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_features, 1):
    plt.subplot(2, 3, i)
    sns.histplot(df[col], bins=30, kde=True, color='skyblue')
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
plt.tight_layout()
plt.show()

#  Boxplots: Numerical Features vs Target (Cardio)
# See if people with heart disease have higher BMI, Age, or BP on average
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_features, 1):
    plt.subplot(2, 3, i)
    sns.boxplot(x='cardio', y=col, data=df, palette='Set2')
    plt.title(f'{col} vs Cardio (0=No, 1=Yes)')
plt.tight_layout()
plt.show()

# 5. Count of One-Hot Encoded Features
# Visualize the frequency of the new binary categories
# We select columns that end with numbers or specific suffixes created by encoding
encoded_cols = [col for col in df.columns if '_' in col and col != 'ap_hi' and col != 'ap_lo']
# (filtering out ap_hi/lo, keeping things like gender_2, cholesterol_3)

if len(encoded_cols) > 0:
    plt.figure(figsize=(12, 6))
    # Summing the values gives the count of '1's for each category
    df[encoded_cols].sum().sort_values().plot(kind='barh', color='teal')
    plt.title('Count of One-Hot Encoded Categories')
    plt.xlabel('Count')
    plt.show()
else:
    print("No encoded columns found to plot counts.")

# Class distribution of Target (cardio)
plt.figure(figsize=(6, 4))
sns.countplot(x='cardio', data=df, palette='pastel')
plt.title('Class Distribution of Cardio (Target)')
plt.show()

"""**TRAIN-TEST SPLIT & SCALING**"""

# Define Features (X) and Target (y)
# Drop 'id' as it is not a predictive feature, and 'cardio' is target
# Also drop 'BMI' raw value if we are using categories, but usually keeping numeric is fine.
# Here we keep 'BMI' as a numeric feature and drop 'id'.
X = df.drop(['id', 'cardio'], axis=1)
y = df['cardio']

# Split: 80% Train, 20% Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Scale features (StandardScaler)
# Ideally, we fit scalar on training data and transform both train and test
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"Train Shape: {X_train_scaled.shape}")
print(f"Test Shape: {X_test_scaled.shape}")

"""**MODEL BUILDING & TUNING**"""

# Define models and parameters
models = {
    'Logistic Regression': {
        'model': LogisticRegression(random_state=42, max_iter=1000),
        'params': {
            'C': [0.1, 1, 10],
            'solver': ['liblinear', 'lbfgs']
        }
    },
    'Decision Tree': {
        'model': DecisionTreeClassifier(random_state=42),
        'params': {
            'max_depth': [5, 10, 20, None],
            'min_samples_split': [2, 10, 20]
        }
    },
    'Random Forest': {
        'model': RandomForestClassifier(random_state=42),
        'params': {
            'n_estimators': [50, 100],
            'max_depth': [5, 10, None],
            'min_samples_split': [2, 10]
        }
    }
}

best_estimators = {}
results_data = []

for model_name, config in models.items():
    print(f"\nTraining {model_name}...")
    grid_search = GridSearchCV(estimator=config['model'],
                               param_grid=config['params'],
                               cv=5,
                               scoring='roc_auc',
                               n_jobs=-1,
                               verbose=1)
    grid_search.fit(X_train_scaled, y_train)

    # Store best model
    best_estimators[model_name] = grid_search.best_estimator_
    print(f"Best Params for {model_name}: {grid_search.best_params_}")
    print(f"Best CV ROC-AUC: {grid_search.best_score_:.4f}")

"""**MODEL EVALUATION**"""

for model_name, model in best_estimators.items():
    # Predictions
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

    # Metrics
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc = roc_auc_score(y_test, y_pred_proba)

    results_data.append({
        'Model': model_name,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1 Score': f1,
        'ROC-AUC': roc
    })

    print(f"\n--- {model_name} Results ---")
    print(f"Accuracy: {acc:.4f}")
    print(f"ROC-AUC: {roc:.4f}")
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

# Comparison Table
results_df = pd.DataFrame(results_data)
print("\n--- Model Comparison Table ---")
print(results_df)

"""**SELECT BEST MODEL**"""

print("\n--- Select Best Model ---")

# Sort by ROC-AUC and F1 Score to find the best
best_model_row = results_df.sort_values(by=['ROC-AUC', 'F1 Score'], ascending=False).iloc[0]
best_model_name = best_model_row['Model']
best_model = best_estimators[best_model_name]

print(f"The best model is: {best_model_name}")
print(f"With ROC-AUC: {best_model_row['ROC-AUC']:.4f}")

"""** PROBABILITY PREDICTION EXAMPLE**

"""

print(" Prediction Example")

# Pick a random sample from the test set
sample_idx = 0
sample_data = X_test_scaled[sample_idx].reshape(1, -1)
true_label = y_test.iloc[sample_idx]

# Predict probability
prob_disease = best_model.predict_proba(sample_data)[0][1]

print(f"For Test Sample Index {sample_idx}:")
print(f"True Label: {'Heart Disease' if true_label==1 else 'No Disease'}")
print(f"Predicted Probability: {prob_disease:.4f}")
print(f"Output: This person has a {prob_disease*100:.2f}% chance of heart disease.")

"""**SAVE AND DOWNLOAD BEST MODEL**"""

# Save model file
model_filename = 'best_cardio_model.pkl'
joblib.dump(best_model, model_filename)
print(f"Model saved as {model_filename}")

# Download file
try:
    files.download(model_filename)
    print("Download started...")
except Exception as e:
    print("Could not download automatically. Please check the file browser in Colab.")

# Save the scaler to a file
joblib.dump(scaler, 'scaler.pkl')

# Download the file to your computer
files.download('scaler.pkl')

files.download('cardio_train.csv')